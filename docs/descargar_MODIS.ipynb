{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Autor:_    __Jesús Casado__ <br> _Revisión:_ __20/11/2019__ <br>\n",
    "\n",
    "__Introducción__<br>\n",
    "Código para descargar datos MODIS del [servidor USGS](https://e4ftl01.cr.usgs.gov/). En dicho enlace se pueden ver las misiones, productos y fechas disponibles.\n",
    "\n",
    "Para poder descargar datos del servidor, es necesario estar registrado en https://urs.earthdata.nasa.gov/.\n",
    "\n",
    "__Cosas que arreglar__ <br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import urllib\n",
    "from http.cookiejar import CookieJar\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EarthdataLogin(username, password, url='https://urs.earthdata.nasa.gov'):\n",
    "    \"\"\"Entra en la cuenta de usuario de Earthdata, con lo que se obtiene permiso para descargar .hdf, y crea un contenedor de las 'cookies' en la sesión.\n",
    "    \n",
    "    Modificado de https://wiki.earthdata.nasa.gov/display/EL/How+To+Access+Data+With+Python\n",
    "    \n",
    "    Entradas:\n",
    "    ---------\n",
    "    username: string. Nombre de usuario en Earthdata\n",
    "    password: string. Contraseña en Earthdata\n",
    "    url:     string. URL del servidor de datos; por defecto 'https://e4ftl01.cr.usgs.gov/'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a password manager to deal with the 401 reponse that is returned from Earthdata Login\n",
    "    password_manager = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    password_manager.add_password(None, \"https://urs.earthdata.nasa.gov\", username, password)\n",
    "    \n",
    "    # Create a cookie jar for storing cookies. This is used to store and return\n",
    "    # the session cookie given to use by the data server (otherwise it will just\n",
    "    # keep sending us back to Earthdata Login to authenticate).  Ideally, we\n",
    "    # should use a file based cookie jar to preserve cookies between runs. This\n",
    "    # will make it much more efficient.\n",
    "    cookie_jar = CookieJar()\n",
    "    \n",
    "    # Install all the handlers.\n",
    "    opener = urllib.request.build_opener(urllib.request.HTTPBasicAuthHandler(password_manager),\n",
    "                                         #urllib.request.HTTPHandler(debuglevel=1),    # Uncomment these two lines to see\n",
    "                                         #urllib.request.HTTPSHandler(debuglevel=1),   # details of the requests/responses\n",
    "                                         urllib.request.HTTPCookieProcessor(cookie_jar))\n",
    "    urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dir(url, ext='/'):\n",
    "    \"\"\"Extrae los directorios existentes en una url.\n",
    "    \n",
    "    Entradas:\n",
    "    ---------\n",
    "    ulr:     string. Dirección url\n",
    "    ext:     string. Caracter a buscar al final de cada nodo de la url para idenficarlo como un nuevo directorio\"\"\"\n",
    "    \n",
    "    # extrae el texto de la url\n",
    "    page = requests.get(url).text\n",
    "    # traduce el texto\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    # extrae las líneas del texto terminadas en 'ext'\n",
    "    list_dir = [url + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
    "    \n",
    "    return list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descarga_MODIS(username, password, path, product, start=None, end=None, tiles=None,\n",
    "                   url='https://e4ftl01.cr.usgs.gov/', format='hdf'):\n",
    "    \"\"\"Descarga los archivos del producto MODIS de interés en las fechas, hojas y formato indicados.\n",
    "    \n",
    "    Entradas:\n",
    "    ---------\n",
    "    username: string. Nombre de usuario en Earthdata\n",
    "    password: string. Contraseña en Earthdata\n",
    "    path:    string. Carpeta donde guardar los datos descargados.\n",
    "    product: string. Producto MODIS. P.ej.: 'MOD16A2.006', 'MYD16A2.006'\n",
    "    start:   string. Fecha a partir de la que descargar datos. Formato 'YYYY-MM-DD'\n",
    "    end:     string. Fecha hasta la que descargar datos. Formato 'YYYY-MM-DD'\n",
    "    tiles:   list of strings. Hojas MODIS a descargar. Formato 'h00v00'\n",
    "    url:     string. URL del servidor de datos; por defecto 'https://e4ftl01.cr.usgs.gov/'\n",
    "    format:  string. Tipo de archivo a descargar: 'hdf', 'jpg', 'xml'\n",
    "    \n",
    "    Salidas:\n",
    "    --------\n",
    "    Se guardan en la carpeta 'path/product' los archivos (hdf, jpg o xml) para la selección indicada.\"\"\"\n",
    "    \n",
    "    # entrar en Earth data con el usuario\n",
    "    EarthdataLogin('casadoj', 'Chomolungma1619', url=url)\n",
    "    \n",
    "    # definir carpeta donde guardar los datos\n",
    "    exportpath = path + '/' + product.split('.')[0] + '/'\n",
    "    if os.path.isdir(exportpath) == False:\n",
    "        os.mkdir(exportpath)\n",
    "    os.chdir(exportpath)\n",
    "    lsdir = os.listdir()\n",
    "    \n",
    "    # url de cada una de las misiones\n",
    "    url_missions = extract_dir(url)\n",
    "    # diccionario con las missiones y sus productos\n",
    "    mission_products = {}\n",
    "    for fd1 in url_missions:\n",
    "        mission = fd1.split('/')[-2]\n",
    "        url_products = extract_dir(fd1)\n",
    "        products = [fd2.split('/')[-2] for fd2 in url_products if fd2.split('/')[-2] != '']\n",
    "        mission_products[mission] = products\n",
    "\n",
    "    # encontrar missión del producto de interés y url del producto\n",
    "    for fd, mission in zip(url_missions, mission_products.keys()):\n",
    "        if product in mission_products[mission]:\n",
    "            urlproduct = fd + product + '/'\n",
    "            break\n",
    "    \n",
    "    # convertir en datetime.date las fechas de inicio y fin de la búsqueda           \n",
    "    if start != None:\n",
    "        start = datetime.strptime(start, '%Y-%m-%d').date()\n",
    "    else:\n",
    "        start = datetime(1950, 1, 1).date()\n",
    "    if end != None:        \n",
    "        end = datetime.strptime(end, '%Y-%m-%d').date()\n",
    "    else:\n",
    "        end = datetime.now().date()\n",
    "        \n",
    "    # seleccionar fechas dentro del periodo de interés\n",
    "    urldates = []\n",
    "    for fd in extract_dir(urlproduct)[1:]:\n",
    "        date = datetime.strptime(fd.split('/')[-2], '%Y.%m.%d').date()\n",
    "        if (start <= date) & (end >= date):\n",
    "            urldates.append(fd)\n",
    "            \n",
    "    for di, urldate in enumerate(urldates):\n",
    "        # seleccionar archivos correspondientes a las hojas de interés\n",
    "        urlfiles = extract_dir(urldate, ext='.' + format)\n",
    "        if tiles == None:\n",
    "            files = [file.split('/')[-1] for file in urlfiles]\n",
    "        else:\n",
    "            files = [file.split('/')[-1] for file in urlfiles if any(tile in file for tile in tiles)]\n",
    "\n",
    "        # descargar archivos\n",
    "        for fi, file in enumerate(files):\n",
    "            print('Fecha {0:>4} de {1:>4}; archivo {2:>3} de {3:>3}'.format(di + 1, len(urldates),\n",
    "                                                                            fi + 1, len(files)),\n",
    "                  end='\\r')\n",
    "            if file in lsdir:\n",
    "                continue\n",
    "            else:\n",
    "                urllib.request.urlretrieve(urldate + file, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rutaMODIS = 'F:/OneDrive - Universidad de Cantabria/Cartografia/MODIS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha  797 de  797; archivo   1 de   1\r"
     ]
    }
   ],
   "source": [
    "# Evapotranspiración (8 días, 500 m) de Aqua\n",
    "descarga_MODIS('casadoj', 'Chomolungma1619', rutaMODIS, 'MYD16A2.006', tiles=['h17v04'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha  866 de  866; archivo   1 de   1\r"
     ]
    }
   ],
   "source": [
    "# Evapotranspiración (8 días, 500 m) de Terra\n",
    "descarga_MODIS('casadoj', 'Chomolungma1619', rutaMODIS, 'MOD16A2.006', tiles=['h17v04'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha  905 de  905; archivo   1 de   1\r"
     ]
    }
   ],
   "source": [
    "# FPAR/LAI de Terra \n",
    "descarga_MODIS('casadoj', 'Chomolungma1619', rutaMODIS, 'MOD15A2H.006', tiles=['h17v04'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha  799 de  799; archivo   1 de   1\r"
     ]
    }
   ],
   "source": [
    "# FPAR/LAI de Aqua\n",
    "descarga_MODIS('casadoj', 'Chomolungma1619', rutaMODIS, 'MYD15A2H.006', tiles=['h17v04'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha  453 de  453; archivo   1 de   1\r"
     ]
    }
   ],
   "source": [
    "# Vegetation indexes NDVI/EVI (16 días, 500m) de Terra ¡hay también a 250 m (MOD13Q1)!\n",
    "descarga_MODIS('casadoj', 'Chomolungma1619', rutaMODIS, 'MOD13A1.006', tiles=['h17v04'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha  399 de  399; archivo   1 de   1\r"
     ]
    }
   ],
   "source": [
    "# Vegetation indexes NDVI/EVI (16 días, 500m) de Aqua ¡hay también a 250 m (MYD13Q1)!\n",
    "descarga_MODIS('casadoj', 'Chomolungma1619', rutaMODIS, 'MYD13A1.006', tiles=['h17v04'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha   18 de   18; archivo   1 de   1\r"
     ]
    }
   ],
   "source": [
    "# Land cover type (anual, 500 m) combinado Aqua y Terra\n",
    "descarga_MODIS('casadoj', 'Chomolungma1619', rutaMODIS, 'MCD12Q1.006', tiles=['h17v04'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha   17 de   17; archivo   1 de   1\r"
     ]
    }
   ],
   "source": [
    "# Land cover dynamics (anual, 500 m) combinado Aqua y Terra\n",
    "descarga_MODIS('casadoj', 'Chomolungma1619', rutaMODIS, 'MCD12Q2.006', tiles=['h17v04'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
